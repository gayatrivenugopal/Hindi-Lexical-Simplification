{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T:\\Research\\Ph.D\\Ph.D\\Work\\HWN API\\JHWNL_1_2\n"
     ]
    }
   ],
   "source": [
    "cd T:\\Research\\Ph.D\\Ph.D\\Work\\HWN API\\JHWNL_1_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Data Cleaning\n",
    "import os\n",
    "import codecs, string, time\n",
    "import pdfminer\n",
    "import sys\n",
    "import subprocess\n",
    "import re\n",
    "import json\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "from pdfminer.pdfinterp import PDFResourceManager, PDFPageInterpreter\n",
    "from pdfminer.converter import TextConverter\n",
    "from pdfminer.layout import LAParams\n",
    "from pdfminer.pdfpage import PDFPage\n",
    "from io import StringIO\n",
    "\n",
    "#Store the list of matra's in a list\n",
    "#Source: http://www.utf8-chartable.de/unicode-utf8-table.pl?start=2304&number=128\n",
    "matras = ['ऽ', 'ँ', 'ं', 'ः', 'ऺ', 'ऻ', '़', 'ा', 'ि', 'ी', 'ु', 'ू', 'ृ', 'ॄ', 'ॅ', 'ॆ', 'े', 'ै', 'ॉ', 'ॊ', 'ो', 'ौ',\n",
    "         '्', 'ॎ', 'ॏ', '॑', '॒', '॓', '॔', 'ॕ', 'ॖ', 'ॗ', 'ॢ', 'ॣ', '॰', 'ॱ', '।', '॥']\n",
    "#separate_matras should not be associated with the same consonant\n",
    "separate_matras = [ 'ऽ', 'ॉ' ,'ा', 'ि', 'ी', 'ु', 'ू','ॆ', 'े', 'ै', 'ॊ', 'ो', 'ौ']\n",
    "\n",
    "\n",
    "#newDF = newDF.append(oldDF, ignore_index = True)\n",
    "#input = PdfFileReader(open(\"Godan_by_Premchand.pdf\", \"rb\"))\n",
    "\n",
    "#for page in input.pages:\n",
    "#    print(page.extractText().encode('UTF-8'))\n",
    "\n",
    "def convert_pdf_to_txt(directory_path, pdf_directory_path, txt_directory_path):\n",
    "    rsrcmgr = PDFResourceManager()\n",
    "    retstr = StringIO()\n",
    "    codec = 'utf-8'\n",
    "    laparams = LAParams()\n",
    "    device = TextConverter(rsrcmgr, retstr, codec=codec, laparams=laparams)\n",
    "    \n",
    "    #read each file in the directory that consists of the PDF files\n",
    "    for pdfpath in os.listdir(directory_path + \"/\" + pdf_directory_path):\n",
    "        with open(directory_path + \"/\" + pdf_directory_path + \"/\" +  pdfpath, 'rb') as fp:\n",
    "            interpreter = PDFPageInterpreter(rsrcmgr, device)\n",
    "            password = \"\"\n",
    "            caching = True\n",
    "            pagenos = set()\n",
    "\n",
    "            for page in PDFPage.get_pages(fp, pagenos, password=password,caching=caching, check_extractable=True):\n",
    "                interpreter.process_page(page)\n",
    "\n",
    "                text = retstr.getvalue()\n",
    "            #write the generated text to the text file that has the same name as that of the pdf file but is stored in\n",
    "            #a different directory\n",
    "            #replace the pdf extension with .txt\n",
    "            file = codecs.open(directory_path + \"/\" + txt_directory_path + \"/\" +  os.path.splitext(os.path.basename(pdfpath))[0] + \".txt\", \"w\", \"utf-8\")\n",
    "            file.write(text)\n",
    "            file.close()\n",
    "\n",
    "    device.close()\n",
    "    retstr.close()\n",
    "    \n",
    "    return 1\n",
    "\n",
    "def read_lines(directory_path, orig_directory_path, clean_directory_path):\n",
    "    #read each file in the directory that consists of the original unprocessed text files\n",
    "    for origpath in os.listdir(directory_path + \"/\" + orig_directory_path):\n",
    "        #create a correponding 'cleaned' text file in a different directory\n",
    "        outfile = codecs.open(directory_path + \"/\" + clean_directory_path + \"/\" + origpath, \"w\", \"utf-8\")\n",
    "        with open(directory_path + \"/\" + orig_directory_path + \"/\" + origpath, encoding = \"utf8\") as rfp:\n",
    "            #read each line of the unprocessed text file\n",
    "            line = rfp.readline()\n",
    "            cnt = 1\n",
    "            while line:\n",
    "                #print(\"Line {}: {}\".format(cnt, line.strip()))\n",
    "                #continue processing the line only if it is not blank\n",
    "                if not line.isspace():\n",
    "                    hindi = 1\n",
    "                    #split the line into words to detect non-Hindi, non-special characters\n",
    "                    wordStructure = line.split()\n",
    "                    for word in wordStructure:\n",
    "                        #hindi is a flag that indicates the the current word is Hindi (or a special character)\n",
    "                        hindi = 1\n",
    "                        #read each letter of the current word\n",
    "                        for letter in word:\n",
    "                            lang = detect_language(letter)\n",
    "                            #set the flag to 0 if the character is not Hindi (or a special character)\n",
    "                            if lang != \"hindi\":\n",
    "                                hindi = 0\n",
    "                                break #stop processing the line\n",
    "                    #if the word is Hindi (or a special character), write the line to the clean file\n",
    "                    if hindi == 1:\n",
    "                        outfile.write(line)\n",
    "                #read the next line\n",
    "                line = rfp.readline()\n",
    "                cnt += 1\n",
    "        outfile.close()\n",
    "    return 1\n",
    "\n",
    "def clean_words(directory_path, clean_directory_path, output_file):\n",
    "    #read each file in the directory that consists of the processed text files\n",
    "    for file in os.listdir(directory_path + \"/\" + clean_directory_path):\n",
    "        \n",
    "        #identify the lines that are incomplete\n",
    "        \n",
    "        if file[-5:] == \"1.txt\":\n",
    "            break\n",
    "        mark_incomplete_lines(directory_path + \"/\" + clean_directory_path + \"/\" + file)\n",
    "        \n",
    "        form_sentences(directory_path + \"/\" + clean_directory_path + \"/\" + file)\n",
    "        #remove the sentences consisting of <end>\n",
    "        clean_sentences(directory_path + \"/\" + clean_directory_path + \"/\" + file)\n",
    "        \n",
    "        #create a dataframe to store the searched words and their corresponding replacements\n",
    "        words_df = pd.DataFrame(columns=['token', 'replacement'])\n",
    "        with open(directory_path + \"/\" + clean_directory_path + \"/\" + file, encoding = \"utf8\") as fp:\n",
    "            #read each line of the unprocessed text file\n",
    "            line = 1\n",
    "            while line:\n",
    "                try:\n",
    "                    line = fp.readline()\n",
    "                    \n",
    "                except:\n",
    "                    #words_df.set_value(len(words_df)-1, 'replacement', 'null')\n",
    "                    continue\n",
    "                #split the line into words\n",
    "                wordStructure = re.split('[`\\-=~—!@#$%^&*()_+\\[\\]{};\\'\\\\:\"|<,./<>? ]',line)\n",
    "                for word in wordStructure:\n",
    "                    \n",
    "                    if word.startswith(\"lengthend\"):\n",
    "                        break\n",
    "                        #TODO: check this change: continue to break\n",
    "                    #not needed: searched_words.set_value(searched_words.size+1, word)\n",
    "                    #store the word in the 'word' column if it does not already exist\n",
    "                    if not words_df.empty:\n",
    "                        #words_df = words_df['token'].map(lambda x: x.encode('utf-8'))\n",
    "                        if not any(words_df.token == word) :\n",
    "                            words_df.set_value(len(words_df), 'token', word)\n",
    "                        else:\n",
    "                        #if the word exists, continue with the next word\n",
    "                                continue\n",
    "                    else:\n",
    "                        words_df.set_value(len(words_df), 'token', word)\n",
    "                    \n",
    "                    \n",
    "                    #search for the word in HWN if it is not a special character\n",
    "                    if not re.search(r'[`\\-=~—!@#$%^&*()_+\\[\\]{};\\'\\\\:\"|<,./<>? ]', word):\n",
    "                        #check if the word is present in HWN\n",
    "                        if get_desc_from_hwn(word, \"hwn data/word.txt\") and word_in_hwn(\"hwn data/word.txt\") == 0:\n",
    "                            #word does not exist in HWN\n",
    "                            #check the words for noisy characters\n",
    "                            status_json = json.loads(read_word(word))\n",
    "                           \n",
    "                            if status_json['status'] == 1: #the modified word exists in HWN\n",
    "                                print (word + \"exists, replacing in file\")\n",
    "                                #store the modified word in the second column of the word in the data frame\n",
    "                                words_df.set_value(len(words_df)-1, 'replacement', status_json['modified_word'])\n",
    "                            elif status_json['status'] == 0:\n",
    "                                print (word + \"does not exist, removing the sentence\")\n",
    "                                #store the value null in the second column of the word in the data frame\n",
    "                                words_df.set_value(len(words_df)-1, 'replacement', 'null')\n",
    "                                #print(words_df);\n",
    "                                #mark the line for removal\n",
    "                                #line = \"<end>\"+ line\n",
    "                                #write this to the file\n",
    "                                #print(line)\n",
    "                                break\n",
    "                            elif status_json['status'] == -1: #status is -1 which means that the word was not modified\n",
    "                                print (word + \"was not modified\")\n",
    "#browse through all the files and search for the words in each file, in HWN.\n",
    "#If it does not exist, find the closest word and replace the current word with this word\n",
    "                            #search for the most similar word\n",
    "                        #line.replace(textToSearch, textToReplace)\n",
    "                #read the next line\n",
    "                #line = fp.readline()\n",
    "        #TODO: remove redundant lines from the file\n",
    "        print (\"Calling modify content for file\")\n",
    "        modify_content_in_file(words_df, directory_path + \"/\" + clean_directory_path + \"/\" + file)\n",
    "    return 1\n",
    "\n",
    "def get_desc_from_hwn(word, output_file):\n",
    "    #write the word to the input file\n",
    "    outfile = codecs.open(\"inputwords.txt\", \"w\", \"utf-8\")\n",
    "    outfile.write(word)\n",
    "    outfile.close()\n",
    "    #clear the contents of the output file\n",
    "    outfile = codecs.open(output_file, \"w\", \"utf-8\")\n",
    "    subprocess.Popen('java -Dfile.encoding=UTF-8 -jar JHWNL.jar', stdout=outfile)\n",
    "    outfile.close()\n",
    "    #check if word was found in HWN\n",
    "    #print(os.stat(output_file).st_size)\n",
    "    return 1\n",
    "\n",
    "def word_in_hwn(file_path):\n",
    "    start = time.time()\n",
    "    while time.time() < start + 3:\n",
    "        time.time()\n",
    "    count = 0\n",
    "    with open(file_path, encoding = \"utf8\") as fp:\n",
    "        try:\n",
    "            line = fp.readline()\n",
    "            while line:\n",
    "                count += 1\n",
    "                try:\n",
    "                    line = fp.readline()\n",
    "                except:\n",
    "                    continue\n",
    "                if count > 10:\n",
    "                    return 1\n",
    "        except:\n",
    "            return 0\n",
    "    return 0\n",
    "\n",
    "def read_word(word):\n",
    "    word = word.strip()\n",
    "    prevword = 0\n",
    "    index = 0\n",
    "    orig_word = word\n",
    "    \n",
    "    #read the word if it is not a matra\n",
    "    if word not in matras:\n",
    "        #check for matras at the beginning of the word because matras are processed after the consonant to which they are attached\n",
    "        if word[:1] in matras:\n",
    "            #if the word begins with a matra, remove the matra\n",
    "            #print (character)\n",
    "            word = word[1:]\n",
    "            \n",
    "            print (\"Original word: \" + orig_word + \" Word: \" + word)\n",
    "        index = 0\n",
    "        old_pos = -1 #used to store the position at which a separate_matra element was found\n",
    "        word = list(word)\n",
    "        \n",
    "        for character in word:\n",
    "            #if not character in matras:\n",
    "                #TODO: check for consonants and special characters; if not consonant or special character, remove the line as it is an invalid character\n",
    "                #if two matras that should not occur together, occur together, mark the first matra for removal\n",
    "                if character in separate_matras:\n",
    "                    if index - old_pos == 1 and not old_pos == -1:\n",
    "                        #mark the previous character for removal\n",
    "                        word[old_pos] = \"R\" #R = remove\n",
    "                    old_pos = index\n",
    "                index += 1\n",
    "        #remove the characters from the word that have been marked for removal\n",
    "        word = \"\".join(word)\n",
    "        word = word.replace(\"R\", \"\")\n",
    "        \n",
    "        #search for the modified word in WordNet\n",
    "        if not orig_word == word:\n",
    "            if get_desc_from_hwn(word, \"hwn data/word.txt\") and word_in_hwn(\"hwn data/word.txt\") == 1:\n",
    "                result = json.dumps({'status': 1, 'modified_word': word}, ensure_ascii=False)\n",
    "            else:\n",
    "                result = json.dumps({'status': 0}, ensure_ascii=False)\n",
    "        else:\n",
    "            result = json.dumps({'status': -1}, ensure_ascii=False)#no change\n",
    "    else:\n",
    "        result = json.dumps({'status': -1}, ensure_ascii=False)#no change\n",
    "    return result\n",
    "        \n",
    "    \n",
    "def mark_incomplete_lines(file):\n",
    "    #add <end> at the end of each line that is to be removed\n",
    "    #find the average length of the lines in the file\n",
    "    print (file)\n",
    "    avg_length = get_avg_length(file)\n",
    "    max_length = len(max(open(file, 'r', encoding = \"utf8\"), key=len))\n",
    "    #read each line and check whether its length is less than the average length\n",
    "    with open(file, encoding = \"utf8\") as oldfile, open(os.path.splitext(file)[0] + \"1.txt\", \"w\", encoding = \"utf8\") as newfile:\n",
    "        for line in oldfile:\n",
    "           # if len(line) < avg_length/2 or (len(line) < max_length*3/4 and line.strip()[-1:] not in [\"।\", \"?\", \"!\"]):\n",
    "            if len(line) < max_length*3/4 and (line.strip()[-1:] not in [\"।\", \"?\", \"!\"] or line.strip()[:1] in matras):\n",
    "                newfile.write(\"lengthend\" + line)\n",
    "            else:\n",
    "                newfile.write(line)\n",
    "    if os.path.isfile(os.path.splitext(file)[0] + \"1.txt\"):\n",
    "        os.remove(file)\n",
    "        os.rename(os.path.splitext(file)[0] + \"1.txt\", os.path.splitext(file)[0] + \".txt\")\n",
    "\n",
    "def get_avg_length(file):\n",
    "    with open(file, \"r\", encoding = \"utf8\") as f:\n",
    "        lines = f.readlines()\n",
    "        return (sum(len(line) for line in lines) / len(lines))\n",
    "    \n",
    "def form_sentences(file):\n",
    "    #create sentences\n",
    "    with open(file, \"r\", encoding = \"utf8\") as oldfile, open(os.path.splitext(file)[0] + \"1.txt\", \"w\", encoding = \"utf8\") as newfile:\n",
    "        nonewlines = oldfile.read().splitlines()\n",
    "        sentences = \" \".join(nonewlines)\n",
    "        sentences = sentences.replace(\"।\", \"।\\n\")\n",
    "        sentences = sentences.replace(\"?\", \"?\\n\")\n",
    "        sentences = sentences.replace(\"!\", \"!\\n\")\n",
    "        #remove the sentences that have <end>\n",
    "        newfile.write(sentences)        \n",
    "    if os.path.isfile(os.path.splitext(file)[0] + \"1.txt\"):\n",
    "        os.remove(file)\n",
    "        os.rename(os.path.splitext(file)[0] + \"1.txt\", os.path.splitext(file)[0] + \".txt\")\n",
    "        \n",
    "def clean_sentences(file):\n",
    "    #create sentences\n",
    "    with open(file, \"r\", encoding = \"utf8\") as oldfile, open(os.path.splitext(file)[0] + \"1.txt\", \"w\", encoding = \"utf8\") as newfile:\n",
    "        lines = oldfile.readlines()\n",
    "        lines = [line.strip() for line in lines if not \"<end>\" in line]\n",
    "        #remove the sentences that have <end>\n",
    "        newfile.write(\"\\n\".join(lines))\n",
    "    if os.path.isfile(os.path.splitext(file)[0] + \"1.txt\"):\n",
    "        os.remove(file)\n",
    "        os.rename(os.path.splitext(file)[0] + \"1.txt\", os.path.splitext(file)[0] + \".txt\")\n",
    "        \n",
    "def modify_content_in_file(words_df,file):\n",
    "    with open(file, \"r\", encoding = \"utf8\") as oldfile, open(os.path.splitext(file)[0] + \"1.txt\", \"w\", encoding = \"utf8\") as newfile:\n",
    "        lines = oldfile.readlines()\n",
    "        for line in lines:\n",
    "            write = 0\n",
    "            words = line.split(\" \")\n",
    "            \n",
    "            for word in words:\n",
    "                if word.startswith(\"lengthend\"):\n",
    "                    break\n",
    "                #row = words_df.loc[words_df['token'] == word]\n",
    "                #print (\"WORD IN FILE\" + word)\n",
    "                row = words_df.loc[words_df['token'] == word]\n",
    "                if not row.empty:\n",
    "                    if row['replacement'].iloc[0] != \"null\":\n",
    "                #words_df.loc[words_df['token'] == word, 'token'].iloc[1] != \"null\":\n",
    "                        #replace the word with its replacement\n",
    "                        #print (words_df.loc[words_df['token'] == word, 'token'])\n",
    "                        #if not pd.isnull(words_df.loc[words_df['token'] == word, 'token']):\n",
    "                        \n",
    "                        ####if words_df.loc[words_df['token'] == word, 'token'] is not None:\n",
    "                        #print(math.isnan(row['replacement'].iloc[0]))\n",
    "                        if type(row['replacement'].iloc[0]) == str: #not math.isnan(row['replacement'].iloc[0]):\n",
    "                            line = line.replace(row['token'].iloc[0], row['replacement'].iloc[0])\n",
    "                        write = 1;\n",
    "                elif re.search(r'[`\\-=~—!@#$%^&*()_+\\[\\]{};\\'\\\\:\"|<,./<>? ]', word):\n",
    "                    write = 1;\n",
    "                else:\n",
    "                    write = 0\n",
    "                    break\n",
    "            if write == 1:\n",
    "                newfile.write(line)\n",
    "                    \n",
    "    if os.path.isfile(os.path.splitext(file)[0] + \"1.txt\"):\n",
    "        os.remove(file)\n",
    "        os.rename(os.path.splitext(file)[0] + \"1.txt\", os.path.splitext(file)[0] + \".txt\")   \n",
    "        \n",
    "    \n",
    "        \n",
    "    #iterate through the dataframe\n",
    "    '''with open(file, \"r\", encoding = \"utf8\") as oldfile, open(os.path.splitext(file)[0] + \"1.txt\", \"w\", encoding = \"utf8\") as newfile:\n",
    "        lines = oldfile.readlines()\n",
    "        for index, row in words_df.iterrows():\n",
    "            print (row['replacement'])\n",
    "            if row['replacement'] != \"null\" and type(row['replacement']) == str:# not math.isnan(row['replacement']):\n",
    "                \n",
    "            #replace the word with its replacement\n",
    "                for line in lines:\n",
    "                    line = line.replace(row['token'], row['replacement'])\n",
    "                    newfile.write(line)    \n",
    "    if os.path.isfile(os.path.splitext(file)[0] + \"1.txt\"):\n",
    "        os.remove(file)\n",
    "        os.rename(os.path.splitext(file)[0] + \"1.txt\", os.path.splitext(file)[0] + \".txt\")\n",
    "      '''  \n",
    "    \n",
    "        \n",
    "        \n",
    "        \n",
    "#Source: https://stackoverflow.com/questions/44474085/how-to-separate-a-only-hindi-script-from-a-file-containing-a-mixture-of-hindi-e\n",
    "def detect_language(character):\n",
    "    maxchar = max(character)\n",
    "    if u'\\u0900' <= maxchar <= u'\\u097f':\n",
    "        return 'hindi'\n",
    "    return 0\n",
    "\n",
    "   \n",
    "#print (convert_pdf_to_txt(\"gaban.pdf\", \"gabanorig.txt\"))\n",
    "#print (convert_pdf_to_txt(\"nirmala.pdf\", \"nirmalaorig.txt\"))\n",
    "#print (read_lines(\"godanorig.txt\", \"godan.txt\"))\n",
    "#print (read_lines(\"gabanorig.txt\", \"gaban.txt\"))\n",
    "#print (read_lines(\"nirmalaorig.txt\", \"nirmala.txt\"))\n",
    "\n",
    "#convert the PDF files to unprocessed text files\n",
    "\n",
    "print (convert_pdf_to_txt(\"corpora\", \"pdf\", \"orig\"))\n",
    "#clean the text files and store in a different directory\n",
    "print (read_lines(\"corpora\", \"orig\", \"clean\"))\n",
    "#get details of a word from the Hindi WordNet\n",
    "#word = \"िदिन\"\n",
    "#read_word(\"s\")\n",
    "clean_words(\"corpora\", \"clean\", \"hwn data/word.txt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T:\\Research\\Ph.D\\Ph.D\\Work\\HWN API\\JHWNL_1_2\\Final Corpora\\bbc-hindiv01.tar\\hindi2vec\n"
     ]
    }
   ],
   "source": [
    "cd T:\\Research\\Ph.D\\Ph.D\\Work\\HWN API\\JHWNL_1_2\\Final Corpora\\bbc-hindiv01.tar\\hindi2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "with open(\"hindi-train.csv\", \"r\", encoding = \"utf8\") as source:\n",
    "    rdr= csv.reader(source, delimiter = '\\t')\n",
    "    with open(\"hindtrain.csv\", \"w\", encoding = \"utf8\") as result:\n",
    "        wtr= csv.writer( result )\n",
    "        for r in rdr:\n",
    "            wtr.writerow( (r[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
